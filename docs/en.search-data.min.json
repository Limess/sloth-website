[{"id":0,"href":"/introduction/","title":"Introduction","parent":"Sloth","content":""},{"id":1,"href":"/usage/cli/","title":"CLI","parent":"Usage","content":"Something\n"},{"id":2,"href":"/specs/default/","title":"Default","parent":"SLO API \u0026 Specs","content":"Something\n"},{"id":3,"href":"/introduction/getting-started/","title":"Getting started","parent":"Introduction","content":"Get sloth and execute with one of the examples:\nsloth generate -i ./examples/getting-started.yml From the spec, you will obtain the rules for Prometheus with the generated SLO recording rules and alert rules.\nSLO spec  version: \u0026#34;prometheus/v1\u0026#34; service: \u0026#34;myservice\u0026#34; labels: owner: \u0026#34;myteam\u0026#34; repo: \u0026#34;myorg/myservice\u0026#34; tier: \u0026#34;2\u0026#34; slos: # We allow failing (5xx and 429) 1 request every 1000 requests (99.9%). - name: \u0026#34;requests-availability\u0026#34; objective: 99.9 description: \u0026#34;Common SLO based on availability for HTTP request responses.\u0026#34; sli: events: error_query: sum(rate(http_request_duration_seconds_count{job=\u0026#34;myservice\u0026#34;,code=~\u0026#34;(5..|429)\u0026#34;}[{{.window}}])) total_query: sum(rate(http_request_duration_seconds_count{job=\u0026#34;myservice\u0026#34;}[{{.window}}])) alerting: name: MyServiceHighErrorRate labels: category: \u0026#34;availability\u0026#34; annotations: # Overwrite default Sloth SLO alert summmary on ticket and page alerts. summary: \u0026#34;High error rate on \u0026#39;myservice\u0026#39; requests responses\u0026#34; page_alert: labels: severity: pageteam routing_key: myteam ticket_alert: labels: severity: \u0026#34;slack\u0026#34; slack_channel: \u0026#34;#alerts-myteam\u0026#34;   Generated  --- # Code generated by Sloth (dev): https://github.com/slok/sloth. # DO NOT EDIT. groups:\n name: sloth-slo-sli-recordings-myservice-requests-availability rules:  record: slo:sli_error:ratio_rate5m expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[5m]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[5m]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 5m tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate30m expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[30m]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[30m]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 30m tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate1h expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[1h]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[1h]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 1h tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate2h expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[2h]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[2h]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 2h tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate6h expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[6h]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[6h]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 6h tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate1d expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[1d]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[1d]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 1d tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate3d expr: |(sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;,code=~\u0026quot;(5..|429)\u0026quot;}[3d]))) / (sum(rate(http_request_duration_seconds_count{job=\u0026quot;myservice\u0026quot;}[3d]))) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability sloth_window: 3d tier: \u0026quot;2\u0026quot; record: slo:sli_error:ratio_rate30d expr: |sum_over_time(slo:sli_error:ratio_rate5m{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;}[30d]) / ignoring (sloth_window) count_over_time(slo:sli_error:ratio_rate5m{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;}[30d]) labels: sloth_window: 30d   name: sloth-slo-meta-recordings-myservice-requests-availability rules:  record: slo:objective:ratio expr: vector(0.9990000000000001) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: slo:error_budget:ratio expr: vector(1-0.9990000000000001) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: slo:time_period:days expr: vector(30) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: slo:current_burn_rate:ratio expr: |slo:sli_error:ratio_rate5m{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} / on(sloth_id, sloth_slo, sloth_service) group_left slo:error_budget:ratio{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: slo:period_burn_rate:ratio expr: |slo:sli_error:ratio_rate30d{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} / on(sloth_id, sloth_slo, sloth_service) group_left slo:error_budget:ratio{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: slo:period_error_budget_remaining:ratio expr: 1 - slo:period_burn_rate:ratio{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_service: myservice sloth_slo: requests-availability tier: \u0026quot;2\u0026quot; record: sloth_slo_info expr: vector(1) labels: cmd: examplesgen.sh owner: myteam repo: myorg/myservice sloth_id: myservice-requests-availability sloth_mode: cli-gen-prom sloth_service: myservice sloth_slo: requests-availability sloth_spec: prometheus/v1 sloth_version: dev tier: \u0026quot;2\u0026quot;   name: sloth-slo-alerts-myservice-requests-availability rules:  alert: MyServiceHighErrorRate expr: |( (slo:sli_error:ratio_rate5m{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (14.4 * 0.0009999999999999432)) and ignoring (sloth_window) (slo:sli_error:ratio_rate1h{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (14.4 * 0.0009999999999999432)) ) or ignoring (sloth_window) ( (slo:sli_error:ratio_rate30m{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (6 * 0.0009999999999999432)) and ignoring (sloth_window) (slo:sli_error:ratio_rate6h{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (6 * 0.0009999999999999432)) ) labels: category: availability routing_key: myteam severity: pageteam sloth_severity: page annotations: summary: High error rate on 'myservice' requests responses title: (page) {{$labels.sloth_service}} {{$labels.sloth_slo}} SLO error budget burn rate is too fast. alert: MyServiceHighErrorRate expr: |( (slo:sli_error:ratio_rate2h{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (3 * 0.0009999999999999432)) and ignoring (sloth_window) (slo:sli_error:ratio_rate1d{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (3 * 0.0009999999999999432)) ) or ignoring (sloth_window) ( (slo:sli_error:ratio_rate6h{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (1 * 0.0009999999999999432)) and ignoring (sloth_window) (slo:sli_error:ratio_rate3d{sloth_id=\u0026quot;myservice-requests-availability\u0026quot;, sloth_service=\u0026quot;myservice\u0026quot;, sloth_slo=\u0026quot;requests-availability\u0026quot;} \u0026gt; (1 * 0.0009999999999999432)) ) labels: category: availability severity: slack slack_channel: '#alerts-myteam' sloth_severity: ticket annotations: summary: High error rate on 'myservice' requests responses title: (ticket) {{$labels.sloth_service}} {{$labels.sloth_slo}} SLO error budget burn rate is too fast.       "},{"id":4,"href":"/introduction/features/","title":"Features","parent":"Introduction","content":" Simple, maintainable and understandable SLO spec. Reliable SLO metrics and alerts. Based on Google SLO implementation and [multi window multi burn][mwmb] alerts framework. Autogenerates Prometheus SLI recording rules in different time windows. Autogenerates Prometheus SLO metadata rules. Autogenerates Prometheus SLO [multi window multi burn][mwmb] alert rules (Page and warning). SLO spec validation (including validate command for Gitops and CI). Customization of labels, disabling different type of alerts\u0026hellip; A single way (uniform) of creating SLOs across all different services and teams. Automatic Grafana dashboard to see all your SLOs state. Single binary and easy to use CLI. Kubernetes (Prometheus-operator) support. Kubernetes Controller/operator mode with CRDs. Support different SLI types. Support for SLI plugins A library with common SLI plugins. [OpenSLO] support.  [OpenSLO]:\n"},{"id":5,"href":"/specs/kubernetes/","title":"Kubernetes (CRD)","parent":"SLO API \u0026 Specs","content":"Something\n"},{"id":6,"href":"/usage/kubernetes/","title":"Kubernetes controller","parent":"Usage","content":"Something\n"},{"id":7,"href":"/usage/plugins/","title":"SLI plugins","parent":"Usage","content":"Something\n"},{"id":8,"href":"/specs/","title":"SLO API \u0026 Specs","parent":"Sloth","content":""},{"id":9,"href":"/introduction/install/","title":"Installing Sloth","parent":"Introduction","content":"Releases    Get binary releases from https://github.com/slok/sloth/releases\nDocker images    Official multi arch docker images in dockerhub.\ndocker pull slok/sloth Building from source code    Clone the repository and build:\ngit clone git@github.com:slok/sloth.git \u0026amp;\u0026amp; \\ cd ./sloth \u0026amp;\u0026amp; \\ make build \u0026amp;\u0026amp; \\ ls -la ./bin Kubernetes     Helm chart Raw Kubernetes manifests without common SLI plugins Raw Kubernetes manifests with common SLI plugins Kustomize  "},{"id":10,"href":"/specs/openslo/","title":"OpenSLO","parent":"SLO API \u0026 Specs","content":"Something\n"},{"id":11,"href":"/architecture/metrics/","title":"Metrics","parent":"Architecture","content":"Something\n"},{"id":12,"href":"/architecture/alerts/","title":"Alerts","parent":"Architecture","content":"Something\n"},{"id":13,"href":"/architecture/","title":"Architecture","parent":"Sloth","content":""},{"id":14,"href":"/dashboards/","title":"Dashboards","parent":"Sloth","content":"Sloth comes with Grafana ready dashboards to be imported and used.\nDetailed SLOs    Get it here\nThis dashboards features detailed view of each of the SLOs.\nEvery SLO will show:\n SLI. SLO metadata details (name, objective\u0026hellip;). SLO Burn rate. Remaining error budget for the current month (Since 1st). Remaining error budget for the last 30 days. In case of enabled alerts, show the state of the current alerts. Error budget month burndown chart. Burn rate magnitude.  Apart from this, the dashboards has a general view of the current exceeded SLO list and graph.\nHigh level overview    Get it here\nThis dashboards shows a high level overview of all the SLOs in the system managed by sloth. Normally this dashboards will be used to check correlation between SLO error budget burns.\nThis dashboards comes with:\n Information of SLOs (quantity, average burn rate of all, triggering alerts\u0026hellip;). Graph and tables of the SLOs currently burning budget at high rates. Timeline with all SLOS error budget burns.  "},{"id":15,"href":"/usage/","title":"Usage","parent":"Sloth","content":""},{"id":16,"href":"/sli-plugins/","title":"SLI plugins","parent":"Sloth","content":"  Group A    Plugin0     Plugin 1     Plugin 2      Group B    Plugin 3      Group C    Plugin 4     Plugin 5       "},{"id":17,"href":"/faq/","title":"F.A.Q","parent":"Sloth","content":"   Why Sloth SLI? SLO? Error budget? Burn rate? SLO based alerting? What are ticket and page alerts? Can I disable alerts? Grafana dashboard? CLI VS K8s controller? SLI types on manifests     Why Sloth    Creating Prometheus rules for SLI/SLO framework is hard, error prone and is pure toil.\nSloth abstracts this task, and we also gain:\n Read friendlyness: Easy to read and declare SLI/SLOs. Gitops: Easy to integrate with CI flows like validation, checks\u0026hellip; Reliability and testing: Generated prometheus rules are already known that work, no need the creation of tests. Centralize features and error fixes: An update in Sloth would be applied to all the SLOs managed/generated with it. Standardize the metrics: Same conventions, automatic dashboards\u0026hellip; Rollout future features for free with the same specs: e.g automatic report creation.  SLI?    Service level indicator. Is a way of quantify how your service should be responding to user.\nTL;DR: What is good/bad service for your users. E.g:\n Requests \u0026gt;=500 considered errors. Requests \u0026gt;200ms considered errors. Process executions with exit code \u0026gt;0 considered errors.  Normally is measured using events: good/bad-events / total-events.\nSLO?    Service level objective. A percent that will tell how many SLI errors your service can have in a specific period of time.\nError budget?    An error budget is the ammount of errors (driven by the SLI) you can have in a specific period of time, this is driven by the SLO.\nLets see an example:\n SLI Error: Requests status code \u0026gt;= 500 Period: 30 days SLO: 99.9% Error budget: 0.0999 (100-99.9) Total requests in 30 days: 10000 Available error requests: 9.99 (10000 * 0.0999 / 100)  If we have more than 9.99 request response with \u0026gt;=500 status code, we would be burning more error budget than the available, if we have less errors, we would end without spending all the error budget.\nBurn rate?    The speed you are consuming your error budget. This is key for SLO based alerting (Sloth will create all these alerts), because depending on the speed you are consuming your error budget, it will trigger your alerts.\nSpeed/rate examples:\n 1: You are consuming 100% of the error budget in the expected period (e.g if 30d period, then 30 days). 2: You are consuming 200% of the error budget in the expected period (e.g if 30d period, then 15 days). 60: You are consuming 6000% of the error budget in the expected period (e.g if 30d period, then 12h hour). 1080: You are consuming 108000% of the error budget in the expected period (e.g if 30d period, then 40 minute).  SLO based alerting?    With SLO based alerting you will get better alerting to a regular alerting system, because:\n Alerts on symptoms (SLIs), not causes. Trigger at different levels (warning/ticket and critical/page). Takes into account time and quantity, this is: speed of errors and number of errors on specific time.  The result of these is:\n Correct time to trigger alerts (important == fast, not so important == slow). Reduce alert fatigue. Reduce false positives and negatives.  What are ticket and page alerts?    MWMB type alerting is based on two kinds of alerts, ticket and page:\n page: Are critical alerts that normally are used to wake up, notify on important channels, trigger oncall\u0026hellip; ticket: The warning alerts that normally open tickets, post messages on non-important Slack channels\u0026hellip;  These are triggered in different ways, page alerts are triggered faster but require faster error budget burn rate, on the other side, ticket alerts are triggered slower and require a lower and constant error budget burn rate.\nCan I disable alerts?    Yes, use disable: true on page and ticket.\nGrafana dashboard?    Check grafana-dashboard, this dashboard will load the SLOs automatically.\nCLI VS K8s controller?    If you don\u0026rsquo;t have Kubernetes and you need raw prometheus rules, its easy, the CLI (generate) mode is the only one that supports raw prometheus rules.\nOn the other side if you have Kubernetes (and most likely prometheus-operator). Using sloth.slok.dev/v1/PrometheusServiceLevel CRD will output the same result used as a CLI or used as a Kubernetes controller.\nThe only difference between bot modes is how Sloth application loads the SLOs manifest. On both modes the output will be a Prometheus Operator Rules CRD.\nBoth have pros and cons:\n The CLI in an advanced gitops flow gives you faster feedback loops because of the generation on the CI. Using as a controller the CRD integrates better in helm charts and similar because it removes that generation extra step. Having the SLO as CRs in K8s, improves the discovery as you can always do kubectl get slos --all-namespaces. The CLI doesn\u0026rsquo;t require an app running, Sloth CRDs registered\u0026hellip; the SLO generation process is simpler, so you have less PoFs.  In a few words, theres no right or wrong answer, pick your own flavour based on your use case: teams size, engineers in the company or development flow\u0026hellip;\nSLI types on manifests    prometheus/v1 (regular) and sloth.slok.dev/v1/PrometheusServiceLevel (Kubernetes CRD), support 3 ways of setting SLIs:\n Events: This are based on 2 queries, the one that returns the total/valid number of events and the one that returns the bad events. Sloht will make a query dividing them to get the final error ratio (0-1). Raw: This is a single raw prometheus query that when executed will return the error ratio (0-1). Plugins: Check plugins for more information. It reference plugins that will be preloaded and already developed. Sloth will execute them on generation and it will return a raw query. This is the best way to abstract queries from users or having SLOs at scale.  "},{"id":18,"href":"/sli-plugins/group1/plugin0/","title":"Plugin0","parent":"Group A","content":"Kubernetes apiserver availability    Availability plugin for the Kubernetes apiserver.\nUses the API HTTP response status codes to measure the events as good or bad. It counts an error event when HTTP response status code is \u0026gt;=500 or 429.\nIn other words, it counts as good events the \u0026lt;500 and !429 HTTP response status codes.\nOptions     filter: (Optional) A prometheus filter string using concatenated labels (e.g: job=\u0026quot;k8sapiserver\u0026quot;,env=\u0026quot;production\u0026quot;,cluster=\u0026quot;k8s-42\u0026quot;)  Metric requirements     apiserver_request_total.  Usage examples    Without filter    sli: plugin: id: \u0026#34;sloth-common/kubernetes/apiserver/availability\u0026#34; With custom filter    sli: plugin: id: \u0026#34;sloth-common/kubernetes/apiserver/availability\u0026#34; options: filter: job=\u0026#34;k8sapiserver\u0026#34;,env=\u0026#34;production\u0026#34;,cluster=\u0026#34;k8s-42\u0026#34; "},{"id":19,"href":"/usage/slo-validation/","title":"Slo Validation","parent":"Usage","content":""},{"id":20,"href":"/sli-plugins/group1/","title":"Group A","parent":"SLI plugins","content":""},{"id":21,"href":"/sli-plugins/group2/","title":"Group B","parent":"SLI plugins","content":""},{"id":22,"href":"/sli-plugins/group3/","title":"Group C","parent":"SLI plugins","content":""},{"id":23,"href":"/sli-plugins/group1/plugin1/","title":"Plugin 1","parent":"Group A","content":"Something\n"},{"id":24,"href":"/sli-plugins/group1/plugin2/","title":"Plugin 2","parent":"Group A","content":"Something\n"},{"id":25,"href":"/sli-plugins/group2/plugin3/","title":"Plugin 3","parent":"Group B","content":"Something\n"},{"id":26,"href":"/sli-plugins/group3/plugin4/","title":"Plugin 4","parent":"Group C","content":"Something\n"},{"id":27,"href":"/sli-plugins/group3/plugin5/","title":"Plugin 5","parent":"Group C","content":"Something\n"},{"id":28,"href":"/","title":"Sloth","parent":"","content":"Stop using complex specs and processes to create Prometheus based SLOs.\nFast, easy and reliable Prometheus SLO generator.\nGet started   Simple  Simplicity and UX is priority. Easy CLIs, APIs and specs that make the usage of Sloth SLOs understandable, maintainable and reliable over time.  Focused on standards  Accepts multiple spec types that adapts to your needs OpenSLO or Kubernetes CRDs that will generate a set of standardized Prometheus metrics and alerts.  Extensible  Sloth can abstract and extend SLIs using plugins. Removing the need to write complex Prometheus queries or copy pasting the same queries over multiple SLOs.   "},{"id":29,"href":"/tags/","title":"Tags","parent":"Sloth","content":""}]